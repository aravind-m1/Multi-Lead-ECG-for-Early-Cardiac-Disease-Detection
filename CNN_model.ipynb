{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Installations & Imports**"
      ],
      "metadata": {
        "id": "bBwOo7UClIHQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_Z0-4zFgTo1"
      },
      "outputs": [],
      "source": [
        "# !pip install --force-reinstall pandas==2.1.4 wfdb --no-cache-dir\n",
        "\n",
        "import os\n",
        "import ast\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import wfdb\n",
        "from scipy.signal import resample\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Device & Config**"
      ],
      "metadata": {
        "id": "diAM6L3BlNb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "class Config:\n",
        "    seed = 2021\n",
        "    device = device\n",
        "\n",
        "    ptbxl_subfolder = '/kaggle/input/ptb-xl-dataset/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1/'\n",
        "    chapman_wfdb_path = '/kaggle/input/chapmanshaoxing-12lead-ecg-database/WFDB_ChapmanShaoxing/'\n",
        "\n",
        "    target_fs = 500\n",
        "    target_length = 5000\n",
        "    num_leads = 12\n",
        "    num_classes = 5\n",
        "\n",
        "    batch_size = 64\n",
        "    num_workers = 4\n",
        "    learning_rate = 1e-3\n",
        "    weight_decay = 1e-5\n",
        "    epochs = 50\n",
        "    patience = 10\n",
        "    grad_clip = 1.0\n",
        "\n",
        "    scheduler_patience = 5\n",
        "    scheduler_factor = 0.5\n",
        "\n",
        "    output_dir = '/kaggle/working/'\n",
        "    model_save_path = os.path.join(output_dir, 'best_cnn_ecg.pth')"
      ],
      "metadata": {
        "id": "sz3bO48mlQOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Seed**"
      ],
      "metadata": {
        "id": "1sz6apC0lS53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(Config.seed)"
      ],
      "metadata": {
        "id": "PrmFU_kulTjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Re-sample**"
      ],
      "metadata": {
        "id": "KcpwwifnlVVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resample_signal(signal, original_length, original_fs):\n",
        "    if original_fs == Config.target_fs and len(signal) == Config.target_length:\n",
        "        return signal\n",
        "\n",
        "    resampled = resample(signal, int(original_length * (Config.target_fs / original_fs)))\n",
        "\n",
        "    if len(resampled) > Config.target_length:\n",
        "        return resampled[:Config.target_length]\n",
        "    elif len(resampled) < Config.target_length:\n",
        "        return np.pad(resampled, (0, Config.target_length - len(resampled)), mode='constant')\n",
        "\n",
        "    return resampled"
      ],
      "metadata": {
        "id": "10cNg_NclWHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PTB-XL Label Map**"
      ],
      "metadata": {
        "id": "97yCPumElYuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_ptbxl_label(scp_str):\n",
        "    scp = ast.literal_eval(scp_str) if isinstance(scp_str, str) else {}\n",
        "\n",
        "    if 'NORM' in scp:\n",
        "        return 0\n",
        "\n",
        "    arrhythmia_keys = {'AFIB','AFL','SVT','SVTAC','AT','VT','TACHY'}\n",
        "    block_keys = {'LBBB','RBBB','AVB','1AVB','2AVB','3AVB'}\n",
        "    hyp_keys = {'LVH','RVH','HYP'}\n",
        "    mi_keys = {'MI','IMI','AMI','ALMI','ISC','ISC_'}\n",
        "\n",
        "    if any(k in scp for k in arrhythmia_keys): return 1\n",
        "    if any(k in scp for k in block_keys): return 2\n",
        "    if any(k in scp for k in hyp_keys): return 3\n",
        "    if any(k in scp for k in mi_keys): return 4\n",
        "\n",
        "    return 4"
      ],
      "metadata": {
        "id": "ektlo9ZElZM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CNN Model**"
      ],
      "metadata": {
        "id": "fnHDdpgflbL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Swish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class ConvNormPool(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, kernel_size):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        self.conv_1 = nn.Conv1d(input_size, hidden_size, kernel_size)\n",
        "        self.conv_2 = nn.Conv1d(hidden_size, hidden_size, kernel_size)\n",
        "        self.conv_3 = nn.Conv1d(hidden_size, hidden_size, kernel_size)\n",
        "\n",
        "        self.norm1 = nn.BatchNorm1d(hidden_size)\n",
        "        self.norm2 = nn.BatchNorm1d(hidden_size)\n",
        "        self.norm3 = nn.BatchNorm1d(hidden_size)\n",
        "\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "        self.swish = Swish()\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = self.conv_1(x)\n",
        "        x = self.swish(self.norm1(conv1))\n",
        "        x = F.pad(x, (self.kernel_size - 1, 0))\n",
        "\n",
        "        x = self.swish(self.norm2(self.conv_2(x)))\n",
        "        x = F.pad(x, (self.kernel_size - 1, 0))\n",
        "\n",
        "        conv3 = self.conv_3(x)\n",
        "        x = self.swish(self.norm3(conv1 + conv3))\n",
        "        x = F.pad(x, (self.kernel_size - 1, 0))\n",
        "\n",
        "        return self.pool(x)\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = ConvNormPool(12, 256, 5)\n",
        "        self.conv2 = ConvNormPool(256, 128, 5)\n",
        "        self.conv3 = ConvNormPool(128, 64, 5)\n",
        "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(64, Config.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "euat2zy9lcK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PTB-XL Loading**"
      ],
      "metadata": {
        "id": "5Eca1zl4l-Yg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading PTB-XL...\")\n",
        "\n",
        "ptbxl_meta = pd.read_csv(os.path.join(Config.ptbxl_subfolder, 'ptbxl_database.csv'))\n",
        "ptbxl_meta['label'] = ptbxl_meta['scp_codes'].apply(map_ptbxl_label)\n",
        "\n",
        "ptbxl_signals = []\n",
        "ptbxl_labels = []\n",
        "\n",
        "for _, row in ptbxl_meta.iterrows():\n",
        "    try:\n",
        "        record_path = os.path.join(Config.ptbxl_subfolder, row['filename_hr'])\n",
        "        record = wfdb.rdrecord(record_path)\n",
        "        signal = record.p_signal.T.astype(np.float32)\n",
        "\n",
        "        if signal.shape[1] != Config.target_length:\n",
        "            signal = np.apply_along_axis(\n",
        "                resample_signal, 1, signal, signal.shape[1], record.fs\n",
        "            )\n",
        "\n",
        "        ptbxl_signals.append(signal)\n",
        "        ptbxl_labels.append(row['label'])\n",
        "\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "ptbxl_signals = np.array(ptbxl_signals)\n",
        "ptbxl_labels = np.array(ptbxl_labels)\n",
        "\n",
        "print(\"PTB-XL loaded:\", len(ptbxl_labels))\n",
        "print(\"PTB-XL label distribution:\", Counter(ptbxl_labels))"
      ],
      "metadata": {
        "id": "xN0Ek9AemAIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chapman DX Extraction**"
      ],
      "metadata": {
        "id": "vQTfElZ2mDyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DX_RE = re.compile(r'^\\s*#?\\s*Dx\\s*[:=]\\s*(.*)$', re.IGNORECASE)\n",
        "\n",
        "def extract_dx_codes_from_header(header):\n",
        "    for c in getattr(header, \"comments\", []) or []:\n",
        "        m = DX_RE.match(str(c).strip())\n",
        "        if m:\n",
        "            return set(re.findall(r\"\\d{5,18}\", m.group(1)))\n",
        "    return set()\n",
        "\n",
        "def extract_dx_codes_from_hea(hea_path):\n",
        "    if not os.path.exists(hea_path):\n",
        "        return set()\n",
        "    with open(hea_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        for ln in f:\n",
        "            m = DX_RE.match(ln.strip())\n",
        "            if m:\n",
        "                return set(re.findall(r\"\\d{5,18}\", m.group(1)))\n",
        "    return set()"
      ],
      "metadata": {
        "id": "KCUiNdmamGYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chapman Taxonomy**"
      ],
      "metadata": {
        "id": "BsbV7PczmIae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NORMAL_CODES = {'426783006'}\n",
        "\n",
        "ARRHYTHMIA_CODES = {\n",
        "    '164889003','164890007','713427006',\n",
        "    '426761007','17338001','284470004'\n",
        "}\n",
        "\n",
        "SINUS_VARIANT_CODES = {\n",
        "    '426177001','427084000','427393009'\n",
        "}\n",
        "\n",
        "BLOCK_CODES = {\n",
        "    '59118001','164909002','270492004',\n",
        "    '713426002','445118002','39732003',\n",
        "    '27885002','6374002','698252002'\n",
        "}\n",
        "\n",
        "HYP_CODES = {\n",
        "    '55827005','164873001','89792004',\n",
        "    '446358003','67741000119109'\n",
        "}\n",
        "\n",
        "STT_MI_CODES = {\n",
        "    '64934002','164931005','429622005',\n",
        "    '428750005','22298006','164865005',\n",
        "    '57054005','426396005','413444003',\n",
        "    '164867002'\n",
        "}\n",
        "\n",
        "OTHER_ABNORMAL_CODES = {\n",
        "    '164917005','251146004','251199005','47665007'\n",
        "}\n",
        "\n",
        "def map_chapman_label(codes, sinus_as_normal=True):\n",
        "\n",
        "    if not codes:\n",
        "        return 0\n",
        "\n",
        "    if codes & STT_MI_CODES:\n",
        "        return 4\n",
        "    if codes & BLOCK_CODES:\n",
        "        return 2\n",
        "    if codes & HYP_CODES:\n",
        "        return 3\n",
        "    if codes & ARRHYTHMIA_CODES:\n",
        "        return 1\n",
        "\n",
        "    if codes & SINUS_VARIANT_CODES:\n",
        "        return 0 if sinus_as_normal else 4\n",
        "\n",
        "    if codes & NORMAL_CODES:\n",
        "        return 0\n",
        "\n",
        "    if codes & OTHER_ABNORMAL_CODES:\n",
        "        return 4\n",
        "\n",
        "    return 4"
      ],
      "metadata": {
        "id": "lHZfrD8ImNY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chapman Loading**"
      ],
      "metadata": {
        "id": "XOsXCw4ImPDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading Chapman dataset...\")\n",
        "\n",
        "chapman_files = [\n",
        "    f[:-4] for f in os.listdir(Config.chapman_wfdb_path)\n",
        "    if f.endswith('.hea')\n",
        "]\n",
        "\n",
        "chapman_signals = []\n",
        "chapman_labels = []\n",
        "\n",
        "for file_name in chapman_files:\n",
        "    try:\n",
        "        full_path = os.path.join(Config.chapman_wfdb_path, file_name)\n",
        "\n",
        "        header = wfdb.rdheader(full_path)\n",
        "        codes = extract_dx_codes_from_header(header)\n",
        "\n",
        "        if not codes:\n",
        "            codes = extract_dx_codes_from_hea(full_path + \".hea\")\n",
        "\n",
        "        label = map_chapman_label(codes, sinus_as_normal=True)\n",
        "\n",
        "        record = wfdb.rdrecord(full_path)\n",
        "        signal = record.p_signal.T.astype(np.float32)\n",
        "\n",
        "        if signal.shape[1] != Config.target_length:\n",
        "            signal = np.apply_along_axis(\n",
        "                resample_signal, 1, signal, signal.shape[1], record.fs\n",
        "            )\n",
        "\n",
        "        chapman_signals.append(signal)\n",
        "        chapman_labels.append(label)\n",
        "\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "chapman_signals = np.array(chapman_signals)\n",
        "chapman_labels = np.array(chapman_labels)\n",
        "\n",
        "print(\"Chapman loaded:\", len(chapman_labels))\n",
        "print(\"Chapman label distribution:\", Counter(chapman_labels))"
      ],
      "metadata": {
        "id": "K0Q_l9j4mVGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train/Val Split**"
      ],
      "metadata": {
        "id": "ugKMQv6UmYJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_val, y_train_val = ptbxl_signals, ptbxl_labels\n",
        "X_test, y_test = chapman_signals, chapman_labels\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val,\n",
        "    y_train_val,\n",
        "    test_size=0.2,\n",
        "    stratify=y_train_val,\n",
        "    random_state=Config.seed\n",
        ")"
      ],
      "metadata": {
        "id": "XnfO-vHtmahv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Normalization**"
      ],
      "metadata": {
        "id": "4fe4vH95mcbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_signals(signals):\n",
        "    mu = signals.mean(axis=2, keepdims=True)\n",
        "    std = signals.std(axis=2, keepdims=True) + 1e-6\n",
        "    return (signals - mu) / std\n",
        "\n",
        "X_train = normalize_signals(X_train)\n",
        "X_val   = normalize_signals(X_val)\n",
        "X_test  = normalize_signals(X_test)\n",
        "\n",
        "print(\"Shapes:\")\n",
        "print(\"Train:\", X_train.shape)\n",
        "print(\"Val:\", X_val.shape)\n",
        "print(\"Test:\", X_test.shape)"
      ],
      "metadata": {
        "id": "ZG6rGcbLmela"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset & Data Loader**"
      ],
      "metadata": {
        "id": "aJ4Q_XLcmgmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, signals, labels):\n",
        "        self.signals = torch.from_numpy(signals).float()\n",
        "        self.labels = torch.from_numpy(labels).long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.signals[idx], self.labels[idx]\n",
        "\n",
        "train_dataset = ECGDataset(X_train, y_train)\n",
        "val_dataset   = ECGDataset(X_val, y_val)\n",
        "test_dataset  = ECGDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=Config.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=Config.num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=Config.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=Config.num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=Config.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=Config.num_workers,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "dqR4oXB2miwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Meter Class**"
      ],
      "metadata": {
        "id": "qZbh7n3Vm0zQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Meter:\n",
        "    def __init__(self, n_classes=Config.num_classes):\n",
        "        self.n_classes = n_classes\n",
        "        self.reset()\n",
        "        self.confusion = torch.zeros((n_classes, n_classes), dtype=torch.long)\n",
        "\n",
        "    def reset(self):\n",
        "        self.metrics = {\n",
        "            \"loss\": 0.0,\n",
        "            \"accuracy\": 0.0,\n",
        "            \"f1\": 0.0,\n",
        "            \"precision\": 0.0,\n",
        "            \"recall\": 0.0,\n",
        "        }\n",
        "        self.total_samples = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, preds, targets, loss):\n",
        "        batch_size = targets.size(0)\n",
        "        self.total_samples += batch_size\n",
        "        self.metrics[\"loss\"] += loss.item() * batch_size\n",
        "\n",
        "        preds_argmax = torch.argmax(preds, dim=1).cpu().numpy()\n",
        "        targets_np = targets.cpu().numpy()\n",
        "\n",
        "        self.metrics[\"accuracy\"] += accuracy_score(targets_np, preds_argmax)\n",
        "        self.metrics[\"f1\"] += f1_score(targets_np, preds_argmax, average=\"macro\", zero_division=0)\n",
        "        self.metrics[\"precision\"] += precision_score(targets_np, preds_argmax, average=\"macro\", zero_division=0)\n",
        "        self.metrics[\"recall\"] += recall_score(targets_np, preds_argmax, average=\"macro\", zero_division=0)\n",
        "\n",
        "        for t, p in zip(targets_np, preds_argmax):\n",
        "            self.confusion[t, p] += 1\n",
        "\n",
        "        self.count += 1\n",
        "\n",
        "    def compute(self):\n",
        "        return {\n",
        "            \"loss\": self.metrics[\"loss\"] / max(1, self.total_samples),\n",
        "            \"accuracy\": self.metrics[\"accuracy\"] / max(1, self.count),\n",
        "            \"f1\": self.metrics[\"f1\"] / max(1, self.count),\n",
        "            \"precision\": self.metrics[\"precision\"] / max(1, self.count),\n",
        "            \"recall\": self.metrics[\"recall\"] / max(1, self.count),\n",
        "        }"
      ],
      "metadata": {
        "id": "-H4JFSMxm4RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Setup**"
      ],
      "metadata": {
        "id": "geKExWzVm6Z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN().to(Config.device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=Config.learning_rate,\n",
        "    weight_decay=Config.weight_decay\n",
        ")\n",
        "\n",
        "scheduler = ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode=\"max\",\n",
        "    factor=Config.scheduler_factor,\n",
        "    patience=Config.scheduler_patience\n",
        ")\n",
        "\n",
        "print(\"Model initialized.\")\n",
        "print(\"Trainable parameters:\",\n",
        "      sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ],
      "metadata": {
        "id": "rOmytHP4m8Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Loop**"
      ],
      "metadata": {
        "id": "NmhcoIE-m-MO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_logs = []\n",
        "val_logs = []\n",
        "\n",
        "best_val_f1 = 0.0\n",
        "best_epoch = -1\n",
        "epochs_no_improve = 0\n",
        "\n",
        "print(\"Starting training...\\n\")\n",
        "\n",
        "for epoch in range(Config.epochs):\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{Config.epochs}\")\n",
        "\n",
        "    # ------------------\n",
        "    # TRAIN\n",
        "    # ------------------\n",
        "    model.train()\n",
        "    train_meter = Meter()\n",
        "\n",
        "    for signals, labels in train_loader:\n",
        "        signals = signals.to(Config.device)\n",
        "        labels = labels.to(Config.device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(signals)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), Config.grad_clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_meter.update(outputs, labels, loss)\n",
        "\n",
        "    train_metrics = train_meter.compute()\n",
        "    train_logs.append(train_metrics)\n",
        "\n",
        "    # ------------------\n",
        "    # VALIDATION\n",
        "    # ------------------\n",
        "    model.eval()\n",
        "    val_meter = Meter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for signals, labels in val_loader:\n",
        "            signals = signals.to(Config.device)\n",
        "            labels = labels.to(Config.device)\n",
        "\n",
        "            outputs = model(signals)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_meter.update(outputs, labels, loss)\n",
        "\n",
        "    val_metrics = val_meter.compute()\n",
        "    val_logs.append(val_metrics)\n",
        "\n",
        "    print(\n",
        "        f\"Train - Loss: {train_metrics['loss']:.4f} | \"\n",
        "        f\"Acc: {train_metrics['accuracy']:.4f} | \"\n",
        "        f\"F1: {train_metrics['f1']:.4f}\"\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Val   - Loss: {val_metrics['loss']:.4f} | \"\n",
        "        f\"Acc: {val_metrics['accuracy']:.4f} | \"\n",
        "        f\"F1: {val_metrics['f1']:.4f}\"\n",
        "    )\n",
        "\n",
        "    scheduler.step(val_metrics[\"f1\"])\n",
        "\n",
        "    # Save best model\n",
        "    if val_metrics[\"f1\"] > best_val_f1:\n",
        "        best_val_f1 = val_metrics[\"f1\"]\n",
        "        best_epoch = epoch + 1\n",
        "        torch.save(model.state_dict(), Config.model_save_path)\n",
        "        print(\"New best model saved.\\n\")\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    if epochs_no_improve >= Config.patience:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "print(\"\\nTraining complete.\")\n",
        "print(\"Best epoch:\", best_epoch)"
      ],
      "metadata": {
        "id": "u7T9T_jmnADq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test Evaluation**"
      ],
      "metadata": {
        "id": "dYx1Q8tjnB6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(Config.model_save_path))\n",
        "model.eval()\n",
        "\n",
        "test_meter = Meter()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for signals, labels in test_loader:\n",
        "        signals = signals.to(Config.device)\n",
        "        labels = labels.to(Config.device)\n",
        "\n",
        "        outputs = model(signals)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        test_meter.update(outputs, labels, loss)\n",
        "\n",
        "test_metrics = test_meter.compute()\n",
        "\n",
        "print(\"\\n===== FINAL CHAPMAN TEST RESULTS =====\")\n",
        "print(\"Loss:\", test_metrics[\"loss\"])\n",
        "print(\"Accuracy:\", test_metrics[\"accuracy\"])\n",
        "print(\"F1 macro:\", test_metrics[\"f1\"])\n",
        "print(\"Precision macro:\", test_metrics[\"precision\"])\n",
        "print(\"Recall macro:\", test_metrics[\"recall\"])"
      ],
      "metadata": {
        "id": "DQC-QaPdnEKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Confusion Matrix**"
      ],
      "metadata": {
        "id": "3urcO307nF_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(\n",
        "    test_meter.confusion.numpy(),\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    cmap=\"Greens\",\n",
        "    xticklabels=['NORM','Arrhythmia','Block','Hypertrophy','MI/Abnormal'],\n",
        "    yticklabels=['NORM','Arrhythmia','Block','Hypertrophy','MI/Abnormal']\n",
        ")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Chapman Test Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-RoVrxWBnIGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Curves**"
      ],
      "metadata": {
        "id": "Yl2Rr3nwnJkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.DataFrame(train_logs)\n",
        "val_df = pd.DataFrame(val_logs)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18,5))\n",
        "\n",
        "axes[0].plot(train_df[\"loss\"], label=\"Train\")\n",
        "axes[0].plot(val_df[\"loss\"], label=\"Val\")\n",
        "axes[0].set_title(\"Loss\")\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(train_df[\"accuracy\"], label=\"Train\")\n",
        "axes[1].plot(val_df[\"accuracy\"], label=\"Val\")\n",
        "axes[1].set_title(\"Accuracy\")\n",
        "axes[1].legend()\n",
        "\n",
        "axes[2].plot(train_df[\"f1\"], label=\"Train\")\n",
        "axes[2].plot(val_df[\"f1\"], label=\"Val\")\n",
        "axes[2].set_title(\"F1 Macro\")\n",
        "axes[2].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HSdyGhhonMAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Save Logs**"
      ],
      "metadata": {
        "id": "VEgU-oDlnOEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logs = pd.concat(\n",
        "    [train_df.add_prefix(\"train_\"),\n",
        "     val_df.add_prefix(\"val_\")],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "logs.to_csv(\"/kaggle/working/training_logs_cnn.csv\", index=False)\n",
        "\n",
        "print(\"Logs saved.\")\n",
        "print(\"Best model saved at:\", Config.model_save_path)"
      ],
      "metadata": {
        "id": "fO0Rw5bPnQD2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}